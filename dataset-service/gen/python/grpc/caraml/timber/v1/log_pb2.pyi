"""
@generated by mypy-protobuf.  Do not edit manually!
isort:skip_file
"""
import builtins
import google.protobuf.descriptor
import google.protobuf.internal.containers
import google.protobuf.internal.enum_type_wrapper
import google.protobuf.message
import typing
import typing_extensions

DESCRIPTOR: google.protobuf.descriptor.FileDescriptor

class _LogType:
    ValueType = typing.NewType('ValueType', builtins.int)
    V: typing_extensions.TypeAlias = ValueType
class _LogTypeEnumTypeWrapper(google.protobuf.internal.enum_type_wrapper._EnumTypeWrapper[_LogType.ValueType], builtins.type):
    DESCRIPTOR: google.protobuf.descriptor.EnumDescriptor
    LOG_TYPE_UNSPECIFIED: _LogType.ValueType  # 0
    LOG_TYPE_PREDICTION: _LogType.ValueType  # 1
    """Log from Prediction service"""

    LOG_TYPE_OBSERVATION: _LogType.ValueType  # 2
    """Log from Observation service"""

    LOG_TYPE_ROUTER: _LogType.ValueType  # 3
    """Log from Router service"""

class LogType(_LogType, metaclass=_LogTypeEnumTypeWrapper):
    """****************************************
    ***********        Log        ************
    *****************************************

    LogType supported by UPI
    """
    pass

LOG_TYPE_UNSPECIFIED: LogType.ValueType  # 0
LOG_TYPE_PREDICTION: LogType.ValueType  # 1
"""Log from Prediction service"""

LOG_TYPE_OBSERVATION: LogType.ValueType  # 2
"""Log from Observation service"""

LOG_TYPE_ROUTER: LogType.ValueType  # 3
"""Log from Router service"""

global___LogType = LogType


class _FluentdOutputType:
    ValueType = typing.NewType('ValueType', builtins.int)
    V: typing_extensions.TypeAlias = ValueType
class _FluentdOutputTypeEnumTypeWrapper(google.protobuf.internal.enum_type_wrapper._EnumTypeWrapper[_FluentdOutputType.ValueType], builtins.type):
    DESCRIPTOR: google.protobuf.descriptor.EnumDescriptor
    FLUENTD_OUTPUT_TYPE_UNSPECIFIED: _FluentdOutputType.ValueType  # 0
    FLUENTD_OUTPUT_TYPE_STDOUT: _FluentdOutputType.ValueType  # 1
    """Fluentd will publish logs to standard output"""

    FLUENTD_OUTPUT_TYPE_BQ: _FluentdOutputType.ValueType  # 2
    """Fluentd will flush logs to BigQuery"""

class FluentdOutputType(_FluentdOutputType, metaclass=_FluentdOutputTypeEnumTypeWrapper):
    """****************************************
    **********        Shared       ***********
    *****************************************

    Data sink where logs would be flushed to via Fluentd
    """
    pass

FLUENTD_OUTPUT_TYPE_UNSPECIFIED: FluentdOutputType.ValueType  # 0
FLUENTD_OUTPUT_TYPE_STDOUT: FluentdOutputType.ValueType  # 1
"""Fluentd will publish logs to standard output"""

FLUENTD_OUTPUT_TYPE_BQ: FluentdOutputType.ValueType  # 2
"""Fluentd will flush logs to BigQuery"""

global___FluentdOutputType = FluentdOutputType


class _KafkaInitialOffset:
    ValueType = typing.NewType('ValueType', builtins.int)
    V: typing_extensions.TypeAlias = ValueType
class _KafkaInitialOffsetEnumTypeWrapper(google.protobuf.internal.enum_type_wrapper._EnumTypeWrapper[_KafkaInitialOffset.ValueType], builtins.type):
    DESCRIPTOR: google.protobuf.descriptor.EnumDescriptor
    KAFKA_INITIAL_OFFSET_UNSPECIFIED: _KafkaInitialOffset.ValueType  # 0
    KAFKA_INITIAL_OFFSET_EARLIEST: _KafkaInitialOffset.ValueType  # 1
    """Automatically reset the offset to the earliest offset"""

    KAFKA_INITIAL_OFFSET_LATEST: _KafkaInitialOffset.ValueType  # 2
    """Automatically reset the offset to the latest offset"""

class KafkaInitialOffset(_KafkaInitialOffset, metaclass=_KafkaInitialOffsetEnumTypeWrapper):
    """Kafka initial offset config"""
    pass

KAFKA_INITIAL_OFFSET_UNSPECIFIED: KafkaInitialOffset.ValueType  # 0
KAFKA_INITIAL_OFFSET_EARLIEST: KafkaInitialOffset.ValueType  # 1
"""Automatically reset the offset to the earliest offset"""

KAFKA_INITIAL_OFFSET_LATEST: KafkaInitialOffset.ValueType  # 2
"""Automatically reset the offset to the latest offset"""

global___KafkaInitialOffset = KafkaInitialOffset


class LogProducer(google.protobuf.message.Message):
    """LogProducer describes the service that generates the log"""
    DESCRIPTOR: google.protobuf.descriptor.Descriptor
    ID_FIELD_NUMBER: builtins.int
    NAME_FIELD_NUMBER: builtins.int
    PROJECT_FIELD_NUMBER: builtins.int
    id: typing.Text
    """Unique identifier of the producer."""

    name: typing.Text
    """Name of the producer, dependent on the type of the log."""

    project: typing.Text
    """Name of the CaraML project that hosts the producer."""

    def __init__(self,
        *,
        id: typing.Text = ...,
        name: typing.Text = ...,
        project: typing.Text = ...,
        ) -> None: ...
    def ClearField(self, field_name: typing_extensions.Literal["id",b"id","name",b"name","project",b"project"]) -> None: ...
global___LogProducer = LogProducer

class Log(google.protobuf.message.Message):
    """Log is an entity/metadata in Dataset Service that represents an append-only
    data produced by ingesting the observation, prediction, or router logs
    """
    DESCRIPTOR: google.protobuf.descriptor.Descriptor
    ID_FIELD_NUMBER: builtins.int
    NAME_FIELD_NUMBER: builtins.int
    TYPE_FIELD_NUMBER: builtins.int
    TARGET_NAMES_FIELD_NUMBER: builtins.int
    BQ_TABLE_FIELD_NUMBER: builtins.int
    LOG_PRODUCER_FIELD_NUMBER: builtins.int
    id: typing.Text
    """Unique identifier of a log generated by a LogProducer."""

    name: typing.Text
    """Name of the log, generated by Dataset Service."""

    type: global___LogType.ValueType
    """Source of the log."""

    @property
    def target_names(self) -> google.protobuf.internal.containers.RepeatedScalarFieldContainer[typing.Text]:
        """List of target names associated with a log."""
        pass
    bq_table: typing.Text
    """BQ table ID where the data is stored."""

    @property
    def log_producer(self) -> global___LogProducer:
        """Details of LogProducer that generated a log."""
        pass
    def __init__(self,
        *,
        id: typing.Text = ...,
        name: typing.Text = ...,
        type: global___LogType.ValueType = ...,
        target_names: typing.Optional[typing.Iterable[typing.Text]] = ...,
        bq_table: typing.Text = ...,
        log_producer: typing.Optional[global___LogProducer] = ...,
        ) -> None: ...
    def HasField(self, field_name: typing_extensions.Literal["log_producer",b"log_producer"]) -> builtins.bool: ...
    def ClearField(self, field_name: typing_extensions.Literal["bq_table",b"bq_table","id",b"id","log_producer",b"log_producer","name",b"name","target_names",b"target_names","type",b"type"]) -> None: ...
global___Log = Log

class FluentdOutputBQConfig(google.protobuf.message.Message):
    """Fluentd BQ Data sink configurations"""
    DESCRIPTOR: google.protobuf.descriptor.Descriptor
    PROJECT_FIELD_NUMBER: builtins.int
    DATASET_FIELD_NUMBER: builtins.int
    TABLE_FIELD_NUMBER: builtins.int
    project: typing.Text
    """GCP Project"""

    dataset: typing.Text
    """GCP Dataset"""

    table: typing.Text
    """GCP Table"""

    def __init__(self,
        *,
        project: typing.Text = ...,
        dataset: typing.Text = ...,
        table: typing.Text = ...,
        ) -> None: ...
    def ClearField(self, field_name: typing_extensions.Literal["dataset",b"dataset","project",b"project","table",b"table"]) -> None: ...
global___FluentdOutputBQConfig = FluentdOutputBQConfig

class FluentdConfig(google.protobuf.message.Message):
    """Fluentd Data sink configurations"""
    DESCRIPTOR: google.protobuf.descriptor.Descriptor
    TYPE_FIELD_NUMBER: builtins.int
    HOST_FIELD_NUMBER: builtins.int
    PORT_FIELD_NUMBER: builtins.int
    TAG_FIELD_NUMBER: builtins.int
    CONFIG_FIELD_NUMBER: builtins.int
    type: global___FluentdOutputType.ValueType
    """The type of Data Sink where Observation logs would be flushed to"""

    host: typing.Text
    """Fluentd Host to connect to"""

    port: builtins.int
    """Fluentd Port to connect to"""

    tag: typing.Text
    """Fluentd Tag to match messages"""

    @property
    def config(self) -> global___FluentdOutputBQConfig: ...
    def __init__(self,
        *,
        type: global___FluentdOutputType.ValueType = ...,
        host: typing.Text = ...,
        port: builtins.int = ...,
        tag: typing.Text = ...,
        config: typing.Optional[global___FluentdOutputBQConfig] = ...,
        ) -> None: ...
    def HasField(self, field_name: typing_extensions.Literal["config",b"config"]) -> builtins.bool: ...
    def ClearField(self, field_name: typing_extensions.Literal["config",b"config","host",b"host","port",b"port","tag",b"tag","type",b"type"]) -> None: ...
global___FluentdConfig = FluentdConfig

class KafkaConfig(google.protobuf.message.Message):
    """Kafka configurations"""
    DESCRIPTOR: google.protobuf.descriptor.Descriptor
    BROKERS_FIELD_NUMBER: builtins.int
    TOPIC_FIELD_NUMBER: builtins.int
    MAX_MESSAGE_BYTES_FIELD_NUMBER: builtins.int
    COMPRESSION_TYPE_FIELD_NUMBER: builtins.int
    CONNECTION_TIMEOUT_FIELD_NUMBER: builtins.int
    POLL_INTERVAL_FIELD_NUMBER: builtins.int
    OFFSET_RESET_FIELD_NUMBER: builtins.int
    brokers: typing.Text
    """Kafka Brokers to connect to, comma-delimited, in the form of "<broker_host>:<broker_port>" """

    topic: typing.Text
    """Kafka Topic to produce to/consume from"""

    max_message_bytes: builtins.int
    """Largest record batch size allowed by Kafka (after compression if compression is enabled)"""

    compression_type: typing.Text
    """The compression type for all data generated by the Producer"""

    connection_timeout: builtins.int
    """ConnectTimeoutMS is the maximum duration (ms) the Kafka Producer/Consumer will block for to get Metadata, before timing out"""

    poll_interval: builtins.int
    """PollInterval is the maximum duration (ms) the Kafka Consumer will block for, before timing out"""

    offset_reset: global___KafkaInitialOffset.ValueType
    """What to do when there is no initial offset in Kafka or if the current offset does not exist any more on the server"""

    def __init__(self,
        *,
        brokers: typing.Text = ...,
        topic: typing.Text = ...,
        max_message_bytes: builtins.int = ...,
        compression_type: typing.Text = ...,
        connection_timeout: builtins.int = ...,
        poll_interval: builtins.int = ...,
        offset_reset: global___KafkaInitialOffset.ValueType = ...,
        ) -> None: ...
    def ClearField(self, field_name: typing_extensions.Literal["brokers",b"brokers","compression_type",b"compression_type","connection_timeout",b"connection_timeout","max_message_bytes",b"max_message_bytes","offset_reset",b"offset_reset","poll_interval",b"poll_interval","topic",b"topic"]) -> None: ...
global___KafkaConfig = KafkaConfig
